# 使用pytorch实现capsule

## 参考
1. [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829v2) 
2. [CapsNet的pytorch实现](https://github.com/dragen1860/CapsNet-Pytorch)

## note
1. 没用gpu训练；使用cpu计算时，在我这里，只能迭代12次，内存就爆了

2. 如果你感兴趣的话，routing这部分还是可以看一看。

3. 我刚刚入门pytorch，代码较烂，建议结合着参考上面第二个链接编写程序。

## 对论文的总结
1. 感觉只是将神经元的意义明确了，即，每个最小单元是个capsule，长度代表它的存在概率，角度代表它的特性。

2. 如果capsule是深度学习未来的方向，那么似乎现有的神经网络框架得修改了，因为现在的最小单元不能是个向量，导致写程序的时候，维度很难搞明白. 卷积操作不是很好做，因为当前库不能自动来做，只能人工手写。

3.  参数变多，因为最小单位都是一个`*8`的向量，如果层数过多，参数会变多，本来这个问题或许可以通过卷积来解决，但是不知道为什么Hinton没有做卷积(如果在中间层有层的话)，或许是效果不好，或者是因为不好写程序，更或许就是单纯的留个坑，让后来人来做。
